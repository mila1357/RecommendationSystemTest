{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a9708e9-7af2-43ab-9767-2eec5371ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next_pages = driver.find_elements(By.CSS_SELECTOR,button_selector)\n",
    "# if next_pages and index < loop_num - 1:\n",
    "#     next_pages[-1].click()\n",
    "#     sleep(random.uniform(wait, wait2))\n",
    "#     soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "# else:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41156b13-3f4f-4700-8651-8a317c5292b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Browser opens\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#Israel War\n",
    "\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.bbc.com/news/topics/c2vdnvdg6xxt'\n",
    "wait = 3\n",
    "wait2 = 5\n",
    "today = datetime.now()\n",
    "\n",
    "def convert_date(text):\n",
    "    if re.search(r\"\\b\\d+ hr\\b|\\b\\d+ hrs\\b\", text):\n",
    "        hours_ago = int(re.search(r\"(\\d+) hr?s?\", text).group(1))\n",
    "        return (today - timedelta(hours=hours_ago)).strftime('%Y-%m-%d')\n",
    "    if re.search(r\"\\b\\d+ day\\b|\\b\\d+ days\\b\", text):\n",
    "        days_ago = int(re.search(r\"(\\d+) day?s?\", text).group(1))\n",
    "        return (today - timedelta(days=days_ago)).strftime('%Y-%m-%d')\n",
    "    if re.search(r\"\\b\\d+ min\\b|\\b\\d+ mins\\b\", text):\n",
    "        minutes_ago = int(re.search(r\"(\\d+) min?s?\", text).group(1))\n",
    "        return (today - timedelta(minutes=minutes_ago)).strftime('%Y-%m-%d')\n",
    "    try:\n",
    "        return datetime.strptime(text, '%d %b %Y').strftime('%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        return text\n",
    "\n",
    "\n",
    "error = False\n",
    "chrome_check = False\n",
    "try:\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    driver.get(\"about:blank\")\n",
    "\n",
    "    driver.implicitly_wait(30)\n",
    "    chrome_check = True\n",
    "    print('Browser opens')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print('Browser does not open')\n",
    "    chrome_check = False\n",
    "\n",
    "\n",
    "news_selector = 'section > div div[data-testid=\"anchor-inner-wrapper\"]'\n",
    "title_selector = 'h2'\n",
    "subtitle_selector = 'p'\n",
    "date_selector = '[data-testid=\"card-metadata-lastupdated\"]'\n",
    "category_selector = '[data-testid=\"card-metadata-tag\"]'\n",
    "button_selector = 'div[data-testid=\"pagination\"] button'\n",
    "link_selector = 'a'\n",
    "\n",
    "\n",
    "if chrome_check:\n",
    "    driver.get(url)\n",
    "    sleep(random.uniform(wait, wait2))\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    button_tags = soup.select(button_selector)\n",
    "    loop_num = int(button_tags[-2].text)\n",
    "    d_list = []\n",
    "    for index in range(loop_num):\n",
    "        news_tags = soup.select(news_selector)\n",
    "        for i, news_tag in enumerate(news_tags):\n",
    "            if i == 2 or i == 5:\n",
    "                continue\n",
    "            title_tag = news_tag.select(title_selector)\n",
    "            subtitle_tag = news_tag.select(subtitle_selector)\n",
    "            date_tag = news_tag.select(date_selector)\n",
    "            category_tag = news_tag.select(category_selector)\n",
    "            link_tag = news_tag.select(link_selector)\n",
    "\n",
    "            title = title_tag[0].text if title_tag else ''\n",
    "            subtitle = subtitle_tag[0].text if subtitle_tag else ''\n",
    "            date = convert_date(date_tag[0].text) if date_tag else today.strftime('%Y-%m-%d')\n",
    "            category = category_tag[0].text if category_tag else ''\n",
    "            link = f\"https://www.bbc.com{link_tag[0]['href']}\" if link_tag else ''\n",
    "\n",
    "            d_list.append({\n",
    "                \"title\": title,\n",
    "                \"subtitle\": subtitle,\n",
    "                \"date\": date,\n",
    "                \"category\": category,\n",
    "                \"link\": link\n",
    "            })\n",
    "        # next_pages = driver.find_elements(By.CSS_SELECTOR,button_selector)\n",
    "        # if next_pages and index < loop_num - 1:\n",
    "        #     next_pages[-1].click()\n",
    "        #     sleep(random.uniform(wait, wait2))\n",
    "        #     soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "        # else:\n",
    "        #     break\n",
    "        try:\n",
    "            next_pages = driver.find_elements(By.CSS_SELECTOR, button_selector)\n",
    "            if next_pages and index < loop_num - 1:\n",
    "                driver.execute_script(\"arguments[0].click();\", next_pages[-1])\n",
    "                sleep(random.uniform(wait, wait2))\n",
    "                soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            else:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error clicking button with JavaScript: {e}\")\n",
    "            break\n",
    "    df_new = pd.DataFrame(d_list)\n",
    "    if os.path.exists('bbc_israelwar.csv'):\n",
    "        df_existing = pd.read_csv('bbc_israelwar.csv')\n",
    "        df_combined = pd.concat([df_existing, df_new])\n",
    "    else:\n",
    "        df_combined = df_new\n",
    "\n",
    "    df_combined = df_combined.drop_duplicates()\n",
    "    df_combined['main_category']='Israel War'\n",
    "    df_combined.to_csv('bbc_israelwar.csv', index=False)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b864b841-1b40-4eef-8ca2-8ba608f211e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Browser opens\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#BBC Ukrainwar\n",
    "\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.bbc.com/news/war-in-ukraine'\n",
    "wait = 3\n",
    "wait2 = 5\n",
    "today = datetime.now()\n",
    "\n",
    "def convert_date(text):\n",
    "    if re.search(r\"\\b\\d+ hr\\b|\\b\\d+ hrs\\b\", text):\n",
    "        hours_ago = int(re.search(r\"(\\d+) hr?s?\", text).group(1))\n",
    "        return (today - timedelta(hours=hours_ago)).strftime('%Y-%m-%d')\n",
    "    if re.search(r\"\\b\\d+ day\\b|\\b\\d+ days\\b\", text):\n",
    "        days_ago = int(re.search(r\"(\\d+) day?s?\", text).group(1))\n",
    "        return (today - timedelta(days=days_ago)).strftime('%Y-%m-%d')\n",
    "    if re.search(r\"\\b\\d+ min\\b|\\b\\d+ mins\\b\", text):\n",
    "        minutes_ago = int(re.search(r\"(\\d+) min?s?\", text).group(1))\n",
    "        return (today - timedelta(minutes=minutes_ago)).strftime('%Y-%m-%d')\n",
    "    try:\n",
    "        return datetime.strptime(text, '%d %b %Y').strftime('%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        return text\n",
    "\n",
    "\n",
    "error = False\n",
    "chrome_check = False\n",
    "try:\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    driver.get(\"about:blank\")\n",
    "\n",
    "    driver.implicitly_wait(30)\n",
    "    chrome_check = True\n",
    "    print('Browser opens')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print('Browser does not open')\n",
    "    chrome_check = False\n",
    "\n",
    "\n",
    "news_selector = 'section > div div[data-testid=\"anchor-inner-wrapper\"]'\n",
    "title_selector = 'h2'\n",
    "subtitle_selector = 'p'\n",
    "date_selector = '[data-testid=\"card-metadata-lastupdated\"]'\n",
    "category_selector = '[data-testid=\"card-metadata-tag\"]'\n",
    "button_selector = 'div[data-testid=\"pagination\"] button'\n",
    "link_selector = 'a'\n",
    "\n",
    "\n",
    "if chrome_check:\n",
    "    driver.get(url)\n",
    "    sleep(random.uniform(wait, wait2))\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    button_tags = soup.select(button_selector)\n",
    "    loop_num = int(button_tags[-2].text)\n",
    "    d_list = []\n",
    "    for index in range(loop_num):\n",
    "        news_tags = soup.select(news_selector)\n",
    "        for i, news_tag in enumerate(news_tags):\n",
    "            if i == 2 or i == 5:\n",
    "                continue\n",
    "            title_tag = news_tag.select(title_selector)\n",
    "            subtitle_tag = news_tag.select(subtitle_selector)\n",
    "            date_tag = news_tag.select(date_selector)\n",
    "            category_tag = news_tag.select(category_selector)\n",
    "            link_tag = news_tag.select(link_selector)\n",
    "\n",
    "            title = title_tag[0].text if title_tag else ''\n",
    "            subtitle = subtitle_tag[0].text if subtitle_tag else ''\n",
    "            date = convert_date(date_tag[0].text) if date_tag else today.strftime('%Y-%m-%d')\n",
    "            category = category_tag[0].text if category_tag else ''\n",
    "            link = f\"https://www.bbc.com{link_tag[0]['href']}\" if link_tag else ''\n",
    "\n",
    "            d_list.append({\n",
    "                \"title\": title,\n",
    "                \"subtitle\": subtitle,\n",
    "                \"date\": date,\n",
    "                \"category\": category,\n",
    "                \"link\": link\n",
    "            })\n",
    "        # next_pages = driver.find_elements(By.CSS_SELECTOR,button_selector)\n",
    "        # if next_pages and index < loop_num - 1:\n",
    "        #     next_pages[-1].click()\n",
    "        #     sleep(random.uniform(wait, wait2))\n",
    "        #     soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "        # else:\n",
    "        #     break\n",
    "        try:\n",
    "            next_pages = driver.find_elements(By.CSS_SELECTOR, button_selector)\n",
    "            if next_pages and index < loop_num - 1:\n",
    "                driver.execute_script(\"arguments[0].click();\", next_pages[-1])\n",
    "                sleep(random.uniform(wait, wait2))\n",
    "                soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            else:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error clicking button with JavaScript: {e}\")\n",
    "            break\n",
    "    df_new = pd.DataFrame(d_list)\n",
    "    if os.path.exists('bbc_ukrain.csv'):\n",
    "        df_existing = pd.read_csv('bbc_ukrain.csv')\n",
    "        df_combined = pd.concat([df_existing, df_new])\n",
    "    else:\n",
    "        df_combined = df_new\n",
    "\n",
    "    df_combined = df_combined.drop_duplicates()\n",
    "    df_combined['main_category']='Ukrain'\n",
    "    df_combined.to_csv('bbc_ukrain.csv', index=False)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f426f1a-272c-40ff-bb89-616472f6c753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Browser opens\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#BBC US&Canada\n",
    "\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.bbc.com/news/us-canada'\n",
    "wait = 3\n",
    "wait2 = 5\n",
    "today = datetime.now()\n",
    "\n",
    "def convert_date(text):\n",
    "    if re.search(r\"\\b\\d+ hr\\b|\\b\\d+ hrs\\b\", text):\n",
    "        hours_ago = int(re.search(r\"(\\d+) hr?s?\", text).group(1))\n",
    "        return (today - timedelta(hours=hours_ago)).strftime('%Y-%m-%d')\n",
    "    if re.search(r\"\\b\\d+ day\\b|\\b\\d+ days\\b\", text):\n",
    "        days_ago = int(re.search(r\"(\\d+) day?s?\", text).group(1))\n",
    "        return (today - timedelta(days=days_ago)).strftime('%Y-%m-%d')\n",
    "    if re.search(r\"\\b\\d+ min\\b|\\b\\d+ mins\\b\", text):\n",
    "        minutes_ago = int(re.search(r\"(\\d+) min?s?\", text).group(1))\n",
    "        return (today - timedelta(minutes=minutes_ago)).strftime('%Y-%m-%d')\n",
    "    try:\n",
    "        return datetime.strptime(text, '%d %b %Y').strftime('%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        return text\n",
    "\n",
    "\n",
    "error = False\n",
    "chrome_check = False\n",
    "try:\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    driver.get(\"about:blank\")\n",
    "\n",
    "    driver.implicitly_wait(30)\n",
    "    chrome_check = True\n",
    "    print('Browser opens')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print('Browser does not open')\n",
    "    chrome_check = False\n",
    "\n",
    "\n",
    "news_selector = 'section > div div[data-testid=\"anchor-inner-wrapper\"]'\n",
    "title_selector = 'h2'\n",
    "subtitle_selector = 'p'\n",
    "date_selector = '[data-testid=\"card-metadata-lastupdated\"]'\n",
    "category_selector = '[data-testid=\"card-metadata-tag\"]'\n",
    "button_selector = 'div[data-testid=\"pagination\"] button'\n",
    "link_selector = 'a'\n",
    "\n",
    "\n",
    "if chrome_check:\n",
    "    driver.get(url)\n",
    "    sleep(random.uniform(wait, wait2))\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    button_tags = soup.select(button_selector)\n",
    "    loop_num = int(button_tags[-2].text)\n",
    "    d_list = []\n",
    "    for index in range(loop_num):\n",
    "        news_tags = soup.select(news_selector)\n",
    "        for i, news_tag in enumerate(news_tags):\n",
    "            if i == 2 or i == 5:\n",
    "                continue\n",
    "            title_tag = news_tag.select(title_selector)\n",
    "            subtitle_tag = news_tag.select(subtitle_selector)\n",
    "            date_tag = news_tag.select(date_selector)\n",
    "            category_tag = news_tag.select(category_selector)\n",
    "            link_tag = news_tag.select(link_selector)\n",
    "\n",
    "            title = title_tag[0].text if title_tag else ''\n",
    "            subtitle = subtitle_tag[0].text if subtitle_tag else ''\n",
    "            date = convert_date(date_tag[0].text) if date_tag else today.strftime('%Y-%m-%d')\n",
    "            category = category_tag[0].text if category_tag else ''\n",
    "            link = f\"https://www.bbc.com{link_tag[0]['href']}\" if link_tag else ''\n",
    "\n",
    "            d_list.append({\n",
    "                \"title\": title,\n",
    "                \"subtitle\": subtitle,\n",
    "                \"date\": date,\n",
    "                \"category\": category,\n",
    "                \"link\": link\n",
    "            })\n",
    "        # next_pages = driver.find_elements(By.CSS_SELECTOR,button_selector)\n",
    "        # if next_pages and index < loop_num - 1:\n",
    "        #     next_pages[-1].click()\n",
    "        #     sleep(random.uniform(wait, wait2))\n",
    "        #     soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "        # else:\n",
    "        #     break\n",
    "        try:\n",
    "            next_pages = driver.find_elements(By.CSS_SELECTOR, button_selector)\n",
    "            if next_pages and index < loop_num - 1:\n",
    "                driver.execute_script(\"arguments[0].click();\", next_pages[-1])\n",
    "                sleep(random.uniform(wait, wait2))\n",
    "                soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            else:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error clicking button with JavaScript: {e}\")\n",
    "            break\n",
    "    df_new = pd.DataFrame(d_list)\n",
    "    if os.path.exists('bbc_us_canada.csv'):\n",
    "        df_existing = pd.read_csv('bbc_us_canada.csv')\n",
    "        df_combined = pd.concat([df_existing, df_new])\n",
    "    else:\n",
    "        df_combined = df_new\n",
    "\n",
    "    df_combined = df_combined.drop_duplicates()\n",
    "    df_combined['main_category']='US & Canada'\n",
    "    df_combined.to_csv('bbc_us_canada.csv', index=False)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53b620b8-5b78-47c9-8444-6f1feea1e05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Browser opens\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#BBC UK\n",
    "\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.bbc.com/news/uk'\n",
    "wait = 3\n",
    "wait2 = 5\n",
    "today = datetime.now()\n",
    "\n",
    "def convert_date(text):\n",
    "    if re.search(r\"\\b\\d+ hr\\b|\\b\\d+ hrs\\b\", text):\n",
    "        hours_ago = int(re.search(r\"(\\d+) hr?s?\", text).group(1))\n",
    "        return (today - timedelta(hours=hours_ago)).strftime('%Y-%m-%d')\n",
    "    if re.search(r\"\\b\\d+ day\\b|\\b\\d+ days\\b\", text):\n",
    "        days_ago = int(re.search(r\"(\\d+) day?s?\", text).group(1))\n",
    "        return (today - timedelta(days=days_ago)).strftime('%Y-%m-%d')\n",
    "    if re.search(r\"\\b\\d+ min\\b|\\b\\d+ mins\\b\", text):\n",
    "        minutes_ago = int(re.search(r\"(\\d+) min?s?\", text).group(1))\n",
    "        return (today - timedelta(minutes=minutes_ago)).strftime('%Y-%m-%d')\n",
    "    try:\n",
    "        return datetime.strptime(text, '%d %b %Y').strftime('%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        return text\n",
    "\n",
    "\n",
    "error = False\n",
    "chrome_check = False\n",
    "try:\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    driver.get(\"about:blank\")\n",
    "\n",
    "    driver.implicitly_wait(30)\n",
    "    chrome_check = True\n",
    "    print('Browser opens')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print('Browser does not open')\n",
    "    chrome_check = False\n",
    "\n",
    "\n",
    "news_selector = 'section > div div[data-testid=\"anchor-inner-wrapper\"]'\n",
    "title_selector = 'h2'\n",
    "subtitle_selector = 'p'\n",
    "date_selector = '[data-testid=\"card-metadata-lastupdated\"]'\n",
    "category_selector = '[data-testid=\"card-metadata-tag\"]'\n",
    "button_selector = 'div[data-testid=\"pagination\"] button'\n",
    "link_selector = 'a'\n",
    "\n",
    "\n",
    "if chrome_check:\n",
    "    driver.get(url)\n",
    "    sleep(random.uniform(wait, wait2))\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    button_tags = soup.select(button_selector)\n",
    "    loop_num = int(button_tags[-2].text)\n",
    "    d_list = []\n",
    "    for index in range(loop_num):\n",
    "        news_tags = soup.select(news_selector)\n",
    "        for i, news_tag in enumerate(news_tags):\n",
    "            if i == 2 or i == 5:\n",
    "                continue\n",
    "            title_tag = news_tag.select(title_selector)\n",
    "            subtitle_tag = news_tag.select(subtitle_selector)\n",
    "            date_tag = news_tag.select(date_selector)\n",
    "            category_tag = news_tag.select(category_selector)\n",
    "            link_tag = news_tag.select(link_selector)\n",
    "\n",
    "            title = title_tag[0].text if title_tag else ''\n",
    "            subtitle = subtitle_tag[0].text if subtitle_tag else ''\n",
    "            date = convert_date(date_tag[0].text) if date_tag else today.strftime('%Y-%m-%d')\n",
    "            category = category_tag[0].text if category_tag else ''\n",
    "            link = f\"https://www.bbc.com{link_tag[0]['href']}\" if link_tag else ''\n",
    "\n",
    "            d_list.append({\n",
    "                \"title\": title,\n",
    "                \"subtitle\": subtitle,\n",
    "                \"date\": date,\n",
    "                \"category\": category,\n",
    "                \"link\": link\n",
    "            })\n",
    "        # next_pages = driver.find_elements(By.CSS_SELECTOR,button_selector)\n",
    "        # if next_pages and index < loop_num - 1:\n",
    "        #     next_pages[-1].click()\n",
    "        #     sleep(random.uniform(wait, wait2))\n",
    "        #     soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "        # else:\n",
    "        #     break\n",
    "        try:\n",
    "            next_pages = driver.find_elements(By.CSS_SELECTOR, button_selector)\n",
    "            if next_pages and index < loop_num - 1:\n",
    "                driver.execute_script(\"arguments[0].click();\", next_pages[-1])\n",
    "                sleep(random.uniform(wait, wait2))\n",
    "                soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            else:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error clicking button with JavaScript: {e}\")\n",
    "            break\n",
    "    df_new = pd.DataFrame(d_list)\n",
    "    if os.path.exists('bbc_uk.csv'):\n",
    "        df_existing = pd.read_csv('bbc_uk.csv')\n",
    "        df_combined = pd.concat([df_existing, df_new])\n",
    "    else:\n",
    "        df_combined = df_new\n",
    "\n",
    "    df_combined = df_combined.drop_duplicates()\n",
    "    df_combined['main_category']='UK'\n",
    "    df_combined.to_csv('bbc_uk.csv', index=False)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8175b588-9a01-42c2-95d8-8f9a5b5dece5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Browser opens\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#Africa\n",
    "\n",
    "\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.bbc.com/news/world/africa'\n",
    "wait = 3\n",
    "wait2 = 5\n",
    "today = datetime.now()\n",
    "\n",
    "def convert_date(text):\n",
    "    if re.search(r\"\\b\\d+ hr\\b|\\b\\d+ hrs\\b\", text):\n",
    "        hours_ago = int(re.search(r\"(\\d+) hr?s?\", text).group(1))\n",
    "        return (today - timedelta(hours=hours_ago)).strftime('%Y-%m-%d')\n",
    "    if re.search(r\"\\b\\d+ day\\b|\\b\\d+ days\\b\", text):\n",
    "        days_ago = int(re.search(r\"(\\d+) day?s?\", text).group(1))\n",
    "        return (today - timedelta(days=days_ago)).strftime('%Y-%m-%d')\n",
    "    if re.search(r\"\\b\\d+ min\\b|\\b\\d+ mins\\b\", text):\n",
    "        minutes_ago = int(re.search(r\"(\\d+) min?s?\", text).group(1))\n",
    "        return (today - timedelta(minutes=minutes_ago)).strftime('%Y-%m-%d')\n",
    "    try:\n",
    "        return datetime.strptime(text, '%d %b %Y').strftime('%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        return text\n",
    "\n",
    "\n",
    "error = False\n",
    "chrome_check = False\n",
    "try:\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    driver.get(\"about:blank\")\n",
    "\n",
    "    driver.implicitly_wait(30)\n",
    "    chrome_check = True\n",
    "    print('Browser opens')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print('Browser does not open')\n",
    "    chrome_check = False\n",
    "\n",
    "\n",
    "news_selector = 'section > div div[data-testid=\"anchor-inner-wrapper\"]'\n",
    "title_selector = 'h2'\n",
    "subtitle_selector = 'p'\n",
    "date_selector = '[data-testid=\"card-metadata-lastupdated\"]'\n",
    "category_selector = '[data-testid=\"card-metadata-tag\"]'\n",
    "button_selector = 'div[data-testid=\"pagination\"] button'\n",
    "link_selector = 'a'\n",
    "\n",
    "\n",
    "if chrome_check:\n",
    "    driver.get(url)\n",
    "    sleep(random.uniform(wait, wait2))\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    button_tags = soup.select(button_selector)\n",
    "    loop_num = int(button_tags[-2].text)\n",
    "    d_list = []\n",
    "    for index in range(loop_num):\n",
    "        news_tags = soup.select(news_selector)\n",
    "        for i, news_tag in enumerate(news_tags):\n",
    "            if i == 2 or i == 5:\n",
    "                continue\n",
    "            title_tag = news_tag.select(title_selector)\n",
    "            subtitle_tag = news_tag.select(subtitle_selector)\n",
    "            date_tag = news_tag.select(date_selector)\n",
    "            category_tag = news_tag.select(category_selector)\n",
    "            link_tag = news_tag.select(link_selector)\n",
    "\n",
    "            title = title_tag[0].text if title_tag else ''\n",
    "            subtitle = subtitle_tag[0].text if subtitle_tag else ''\n",
    "            date = convert_date(date_tag[0].text) if date_tag else today.strftime('%Y-%m-%d')\n",
    "            category = category_tag[0].text if category_tag else ''\n",
    "            link = f\"https://www.bbc.com{link_tag[0]['href']}\" if link_tag else ''\n",
    "\n",
    "            d_list.append({\n",
    "                \"title\": title,\n",
    "                \"subtitle\": subtitle,\n",
    "                \"date\": date,\n",
    "                \"category\": category,\n",
    "                \"link\": link\n",
    "            })\n",
    "        # next_pages = driver.find_elements(By.CSS_SELECTOR,button_selector)\n",
    "        # if next_pages and index < loop_num - 1:\n",
    "        #     next_pages[-1].click()\n",
    "        #     sleep(random.uniform(wait, wait2))\n",
    "        #     soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "        # else:\n",
    "        #     break\n",
    "        try:\n",
    "            next_pages = driver.find_elements(By.CSS_SELECTOR, button_selector)\n",
    "            if next_pages and index < loop_num - 1:\n",
    "                driver.execute_script(\"arguments[0].click();\", next_pages[-1])\n",
    "                sleep(random.uniform(wait, wait2))\n",
    "                soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            else:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error clicking button with JavaScript: {e}\")\n",
    "            break\n",
    "    df_new = pd.DataFrame(d_list)\n",
    "    if os.path.exists('bbc_africa.csv'):\n",
    "        df_existing = pd.read_csv('bbc_africa.csv')\n",
    "        df_combined = pd.concat([df_existing, df_new])\n",
    "    else:\n",
    "        df_combined = df_new\n",
    "\n",
    "    df_combined = df_combined.drop_duplicates()\n",
    "    df_combined['main_category']='Africa'\n",
    "    df_combined.to_csv('bbc_africa.csv', index=False)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d4ba68c-acd6-4702-b193-253c958ba136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Browser opens\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#Asia\n",
    "\n",
    "\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.bbc.com/news/world/asia'\n",
    "wait = 3\n",
    "wait2 = 5\n",
    "today = datetime.now()\n",
    "\n",
    "def convert_date(text):\n",
    "    if re.search(r\"\\b\\d+ hr\\b|\\b\\d+ hrs\\b\", text):\n",
    "        hours_ago = int(re.search(r\"(\\d+) hr?s?\", text).group(1))\n",
    "        return (today - timedelta(hours=hours_ago)).strftime('%Y-%m-%d')\n",
    "    if re.search(r\"\\b\\d+ day\\b|\\b\\d+ days\\b\", text):\n",
    "        days_ago = int(re.search(r\"(\\d+) day?s?\", text).group(1))\n",
    "        return (today - timedelta(days=days_ago)).strftime('%Y-%m-%d')\n",
    "    if re.search(r\"\\b\\d+ min\\b|\\b\\d+ mins\\b\", text):\n",
    "        minutes_ago = int(re.search(r\"(\\d+) min?s?\", text).group(1))\n",
    "        return (today - timedelta(minutes=minutes_ago)).strftime('%Y-%m-%d')\n",
    "    try:\n",
    "        return datetime.strptime(text, '%d %b %Y').strftime('%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        return text\n",
    "\n",
    "\n",
    "error = False\n",
    "chrome_check = False\n",
    "try:\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    driver.get(\"about:blank\")\n",
    "\n",
    "    driver.implicitly_wait(30)\n",
    "    chrome_check = True\n",
    "    print('Browser opens')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print('Browser does not open')\n",
    "    chrome_check = False\n",
    "\n",
    "\n",
    "news_selector = 'section > div div[data-testid=\"anchor-inner-wrapper\"]'\n",
    "title_selector = 'h2'\n",
    "subtitle_selector = 'p'\n",
    "date_selector = '[data-testid=\"card-metadata-lastupdated\"]'\n",
    "category_selector = '[data-testid=\"card-metadata-tag\"]'\n",
    "button_selector = 'div[data-testid=\"pagination\"] button'\n",
    "link_selector = 'a'\n",
    "\n",
    "\n",
    "if chrome_check:\n",
    "    driver.get(url)\n",
    "    sleep(random.uniform(wait, wait2))\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    button_tags = soup.select(button_selector)\n",
    "    loop_num = int(button_tags[-2].text)\n",
    "    d_list = []\n",
    "    for index in range(loop_num):\n",
    "        news_tags = soup.select(news_selector)\n",
    "        for i, news_tag in enumerate(news_tags):\n",
    "            if i == 2 or i == 5:\n",
    "                continue\n",
    "            title_tag = news_tag.select(title_selector)\n",
    "            subtitle_tag = news_tag.select(subtitle_selector)\n",
    "            date_tag = news_tag.select(date_selector)\n",
    "            category_tag = news_tag.select(category_selector)\n",
    "            link_tag = news_tag.select(link_selector)\n",
    "\n",
    "            title = title_tag[0].text if title_tag else ''\n",
    "            subtitle = subtitle_tag[0].text if subtitle_tag else ''\n",
    "            date = convert_date(date_tag[0].text) if date_tag else today.strftime('%Y-%m-%d')\n",
    "            category = category_tag[0].text if category_tag else ''\n",
    "            link = f\"https://www.bbc.com{link_tag[0]['href']}\" if link_tag else ''\n",
    "\n",
    "            d_list.append({\n",
    "                \"title\": title,\n",
    "                \"subtitle\": subtitle,\n",
    "                \"date\": date,\n",
    "                \"category\": category,\n",
    "                \"link\": link\n",
    "            })\n",
    "        # next_pages = driver.find_elements(By.CSS_SELECTOR,button_selector)\n",
    "        # if next_pages and index < loop_num - 1:\n",
    "        #     next_pages[-1].click()\n",
    "        #     sleep(random.uniform(wait, wait2))\n",
    "        #     soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "        # else:\n",
    "        #     break\n",
    "        try:\n",
    "            next_pages = driver.find_elements(By.CSS_SELECTOR, button_selector)\n",
    "            if next_pages and index < loop_num - 1:\n",
    "                driver.execute_script(\"arguments[0].click();\", next_pages[-1])\n",
    "                sleep(random.uniform(wait, wait2))\n",
    "                soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            else:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error clicking button with JavaScript: {e}\")\n",
    "            break\n",
    "    df_new = pd.DataFrame(d_list)\n",
    "    if os.path.exists('bbc_asia.csv'):\n",
    "        df_existing = pd.read_csv('bbc_asia.csv')\n",
    "        df_combined = pd.concat([df_existing, df_new])\n",
    "    else:\n",
    "        df_combined = df_new\n",
    "\n",
    "    df_combined = df_combined.drop_duplicates()\n",
    "    df_combined['main_category']='Asia'\n",
    "    df_combined.to_csv('bbc_asia.csv', index=False)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a0efd17-77cc-456c-a103-c6bf5da093cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Browser opens\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#Australia\n",
    "\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.bbc.com/news/world/australia'\n",
    "wait = 3\n",
    "wait2 = 5\n",
    "today = datetime.now()\n",
    "\n",
    "def convert_date(text):\n",
    "    if re.search(r\"\\b\\d+ hr\\b|\\b\\d+ hrs\\b\", text):\n",
    "        hours_ago = int(re.search(r\"(\\d+) hr?s?\", text).group(1))\n",
    "        return (today - timedelta(hours=hours_ago)).strftime('%Y-%m-%d')\n",
    "    if re.search(r\"\\b\\d+ day\\b|\\b\\d+ days\\b\", text):\n",
    "        days_ago = int(re.search(r\"(\\d+) day?s?\", text).group(1))\n",
    "        return (today - timedelta(days=days_ago)).strftime('%Y-%m-%d')\n",
    "    if re.search(r\"\\b\\d+ min\\b|\\b\\d+ mins\\b\", text):\n",
    "        minutes_ago = int(re.search(r\"(\\d+) min?s?\", text).group(1))\n",
    "        return (today - timedelta(minutes=minutes_ago)).strftime('%Y-%m-%d')\n",
    "    try:\n",
    "        return datetime.strptime(text, '%d %b %Y').strftime('%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        return text\n",
    "\n",
    "\n",
    "error = False\n",
    "chrome_check = False\n",
    "try:\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    driver.get(\"about:blank\")\n",
    "\n",
    "    driver.implicitly_wait(30)\n",
    "    chrome_check = True\n",
    "    print('Browser opens')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print('Browser does not open')\n",
    "    chrome_check = False\n",
    "\n",
    "\n",
    "news_selector = 'section > div div[data-testid=\"anchor-inner-wrapper\"]'\n",
    "title_selector = 'h2'\n",
    "subtitle_selector = 'p'\n",
    "date_selector = '[data-testid=\"card-metadata-lastupdated\"]'\n",
    "category_selector = '[data-testid=\"card-metadata-tag\"]'\n",
    "button_selector = 'div[data-testid=\"pagination\"] button'\n",
    "link_selector = 'a'\n",
    "\n",
    "\n",
    "if chrome_check:\n",
    "    driver.get(url)\n",
    "    sleep(random.uniform(wait, wait2))\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    button_tags = soup.select(button_selector)\n",
    "    loop_num = int(button_tags[-2].text)\n",
    "    d_list = []\n",
    "    for index in range(loop_num):\n",
    "        news_tags = soup.select(news_selector)\n",
    "        for i, news_tag in enumerate(news_tags):\n",
    "            if i == 2 or i == 5:\n",
    "                continue\n",
    "            title_tag = news_tag.select(title_selector)\n",
    "            subtitle_tag = news_tag.select(subtitle_selector)\n",
    "            date_tag = news_tag.select(date_selector)\n",
    "            category_tag = news_tag.select(category_selector)\n",
    "            link_tag = news_tag.select(link_selector)\n",
    "\n",
    "            title = title_tag[0].text if title_tag else ''\n",
    "            subtitle = subtitle_tag[0].text if subtitle_tag else ''\n",
    "            date = convert_date(date_tag[0].text) if date_tag else today.strftime('%Y-%m-%d')\n",
    "            category = category_tag[0].text if category_tag else ''\n",
    "            link = f\"https://www.bbc.com{link_tag[0]['href']}\" if link_tag else ''\n",
    "\n",
    "            d_list.append({\n",
    "                \"title\": title,\n",
    "                \"subtitle\": subtitle,\n",
    "                \"date\": date,\n",
    "                \"category\": category,\n",
    "                \"link\": link\n",
    "            })\n",
    "        # next_pages = driver.find_elements(By.CSS_SELECTOR,button_selector)\n",
    "        # if next_pages and index < loop_num - 1:\n",
    "        #     next_pages[-1].click()\n",
    "        #     sleep(random.uniform(wait, wait2))\n",
    "        #     soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "        # else:\n",
    "        #     break\n",
    "        try:\n",
    "            next_pages = driver.find_elements(By.CSS_SELECTOR, button_selector)\n",
    "            if next_pages and index < loop_num - 1:\n",
    "                driver.execute_script(\"arguments[0].click();\", next_pages[-1])\n",
    "                sleep(random.uniform(wait, wait2))\n",
    "                soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            else:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error clicking button with JavaScript: {e}\")\n",
    "            break\n",
    "    df_new = pd.DataFrame(d_list)\n",
    "    if os.path.exists('bbc_australia.csv'):\n",
    "        df_existing = pd.read_csv('bbc_australia.csv')\n",
    "        df_combined = pd.concat([df_existing, df_new])\n",
    "    else:\n",
    "        df_combined = df_new\n",
    "\n",
    "    df_combined = df_combined.drop_duplicates()\n",
    "    df_combined['main_category']='Australia'\n",
    "    df_combined.to_csv('bbc_australia.csv', index=False)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59ec4d60-4ea0-4102-83cb-c839565e70db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Browser opens\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#Europe\n",
    "\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.bbc.com/news/world/europe'\n",
    "wait = 3\n",
    "wait2 = 5\n",
    "today = datetime.now()\n",
    "\n",
    "def convert_date(text):\n",
    "    if re.search(r\"\\b\\d+ hr\\b|\\b\\d+ hrs\\b\", text):\n",
    "        hours_ago = int(re.search(r\"(\\d+) hr?s?\", text).group(1))\n",
    "        return (today - timedelta(hours=hours_ago)).strftime('%Y-%m-%d')\n",
    "    if re.search(r\"\\b\\d+ day\\b|\\b\\d+ days\\b\", text):\n",
    "        days_ago = int(re.search(r\"(\\d+) day?s?\", text).group(1))\n",
    "        return (today - timedelta(days=days_ago)).strftime('%Y-%m-%d')\n",
    "    if re.search(r\"\\b\\d+ min\\b|\\b\\d+ mins\\b\", text):\n",
    "        minutes_ago = int(re.search(r\"(\\d+) min?s?\", text).group(1))\n",
    "        return (today - timedelta(minutes=minutes_ago)).strftime('%Y-%m-%d')\n",
    "    try:\n",
    "        return datetime.strptime(text, '%d %b %Y').strftime('%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        return text\n",
    "\n",
    "\n",
    "error = False\n",
    "chrome_check = False\n",
    "try:\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    driver.get(\"about:blank\")\n",
    "\n",
    "    driver.implicitly_wait(30)\n",
    "    chrome_check = True\n",
    "    print('Browser opens')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print('Browser does not open')\n",
    "    chrome_check = False\n",
    "\n",
    "\n",
    "news_selector = 'section > div div[data-testid=\"anchor-inner-wrapper\"]'\n",
    "title_selector = 'h2'\n",
    "subtitle_selector = 'p'\n",
    "date_selector = '[data-testid=\"card-metadata-lastupdated\"]'\n",
    "category_selector = '[data-testid=\"card-metadata-tag\"]'\n",
    "button_selector = 'div[data-testid=\"pagination\"] button'\n",
    "link_selector = 'a'\n",
    "\n",
    "\n",
    "if chrome_check:\n",
    "    driver.get(url)\n",
    "    sleep(random.uniform(wait, wait2))\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    button_tags = soup.select(button_selector)\n",
    "    loop_num = int(button_tags[-2].text)\n",
    "    d_list = []\n",
    "    for index in range(loop_num):\n",
    "        news_tags = soup.select(news_selector)\n",
    "        for i, news_tag in enumerate(news_tags):\n",
    "            if i == 2 or i == 5:\n",
    "                continue\n",
    "            title_tag = news_tag.select(title_selector)\n",
    "            subtitle_tag = news_tag.select(subtitle_selector)\n",
    "            date_tag = news_tag.select(date_selector)\n",
    "            category_tag = news_tag.select(category_selector)\n",
    "            link_tag = news_tag.select(link_selector)\n",
    "\n",
    "            title = title_tag[0].text if title_tag else ''\n",
    "            subtitle = subtitle_tag[0].text if subtitle_tag else ''\n",
    "            date = convert_date(date_tag[0].text) if date_tag else today.strftime('%Y-%m-%d')\n",
    "            category = category_tag[0].text if category_tag else ''\n",
    "            link = f\"https://www.bbc.com{link_tag[0]['href']}\" if link_tag else ''\n",
    "\n",
    "            d_list.append({\n",
    "                \"title\": title,\n",
    "                \"subtitle\": subtitle,\n",
    "                \"date\": date,\n",
    "                \"category\": category,\n",
    "                \"link\": link\n",
    "            })\n",
    "        # next_pages = driver.find_elements(By.CSS_SELECTOR,button_selector)\n",
    "        # if next_pages and index < loop_num - 1:\n",
    "        #     next_pages[-1].click()\n",
    "        #     sleep(random.uniform(wait, wait2))\n",
    "        #     soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "        # else:\n",
    "        #     break\n",
    "        try:\n",
    "            next_pages = driver.find_elements(By.CSS_SELECTOR, button_selector)\n",
    "            if next_pages and index < loop_num - 1:\n",
    "                driver.execute_script(\"arguments[0].click();\", next_pages[-1])\n",
    "                sleep(random.uniform(wait, wait2))\n",
    "                soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            else:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error clicking button with JavaScript: {e}\")\n",
    "            break\n",
    "    df_new = pd.DataFrame(d_list)\n",
    "    if os.path.exists('bbc_europe.csv'):\n",
    "        df_existing = pd.read_csv('bbc_europe.csv')\n",
    "        df_combined = pd.concat([df_existing, df_new])\n",
    "    else:\n",
    "        df_combined = df_new\n",
    "\n",
    "    df_combined = df_combined.drop_duplicates()\n",
    "    df_combined['main_category']='Europe'\n",
    "    df_combined.to_csv('bbc_europe.csv', index=False)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0103426a-4309-40aa-8290-21344c86a53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Browser opens\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#Latin America\n",
    "\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.bbc.com/news/world/latin_america'\n",
    "wait = 3\n",
    "wait2 = 5\n",
    "today = datetime.now()\n",
    "\n",
    "def convert_date(text):\n",
    "    if re.search(r\"\\b\\d+ hr\\b|\\b\\d+ hrs\\b\", text):\n",
    "        hours_ago = int(re.search(r\"(\\d+) hr?s?\", text).group(1))\n",
    "        return (today - timedelta(hours=hours_ago)).strftime('%Y-%m-%d')\n",
    "    if re.search(r\"\\b\\d+ day\\b|\\b\\d+ days\\b\", text):\n",
    "        days_ago = int(re.search(r\"(\\d+) day?s?\", text).group(1))\n",
    "        return (today - timedelta(days=days_ago)).strftime('%Y-%m-%d')\n",
    "    if re.search(r\"\\b\\d+ min\\b|\\b\\d+ mins\\b\", text):\n",
    "        minutes_ago = int(re.search(r\"(\\d+) min?s?\", text).group(1))\n",
    "        return (today - timedelta(minutes=minutes_ago)).strftime('%Y-%m-%d')\n",
    "    try:\n",
    "        return datetime.strptime(text, '%d %b %Y').strftime('%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        return text\n",
    "\n",
    "\n",
    "error = False\n",
    "chrome_check = False\n",
    "try:\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    driver.get(\"about:blank\")\n",
    "\n",
    "    driver.implicitly_wait(30)\n",
    "    chrome_check = True\n",
    "    print('Browser opens')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print('Browser does not open')\n",
    "    chrome_check = False\n",
    "\n",
    "\n",
    "news_selector = 'section > div div[data-testid=\"anchor-inner-wrapper\"]'\n",
    "title_selector = 'h2'\n",
    "subtitle_selector = 'p'\n",
    "date_selector = '[data-testid=\"card-metadata-lastupdated\"]'\n",
    "category_selector = '[data-testid=\"card-metadata-tag\"]'\n",
    "button_selector = 'div[data-testid=\"pagination\"] button'\n",
    "link_selector = 'a'\n",
    "\n",
    "\n",
    "if chrome_check:\n",
    "    driver.get(url)\n",
    "    sleep(random.uniform(wait, wait2))\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    button_tags = soup.select(button_selector)\n",
    "    loop_num = int(button_tags[-2].text)\n",
    "    d_list = []\n",
    "    for index in range(loop_num):\n",
    "        news_tags = soup.select(news_selector)\n",
    "        for i, news_tag in enumerate(news_tags):\n",
    "            if i == 2 or i == 5:\n",
    "                continue\n",
    "            title_tag = news_tag.select(title_selector)\n",
    "            subtitle_tag = news_tag.select(subtitle_selector)\n",
    "            date_tag = news_tag.select(date_selector)\n",
    "            category_tag = news_tag.select(category_selector)\n",
    "            link_tag = news_tag.select(link_selector)\n",
    "\n",
    "            title = title_tag[0].text if title_tag else ''\n",
    "            subtitle = subtitle_tag[0].text if subtitle_tag else ''\n",
    "            date = convert_date(date_tag[0].text) if date_tag else today.strftime('%Y-%m-%d')\n",
    "            category = category_tag[0].text if category_tag else ''\n",
    "            link = f\"https://www.bbc.com{link_tag[0]['href']}\" if link_tag else ''\n",
    "\n",
    "            d_list.append({\n",
    "                \"title\": title,\n",
    "                \"subtitle\": subtitle,\n",
    "                \"date\": date,\n",
    "                \"category\": category,\n",
    "                \"link\": link\n",
    "            })\n",
    "        # next_pages = driver.find_elements(By.CSS_SELECTOR,button_selector)\n",
    "        # if next_pages and index < loop_num - 1:\n",
    "        #     next_pages[-1].click()\n",
    "        #     sleep(random.uniform(wait, wait2))\n",
    "        #     soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "        # else:\n",
    "        #     break\n",
    "        try:\n",
    "            next_pages = driver.find_elements(By.CSS_SELECTOR, button_selector)\n",
    "            if next_pages and index < loop_num - 1:\n",
    "                driver.execute_script(\"arguments[0].click();\", next_pages[-1])\n",
    "                sleep(random.uniform(wait, wait2))\n",
    "                soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            else:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error clicking button with JavaScript: {e}\")\n",
    "            break\n",
    "    df_new = pd.DataFrame(d_list)\n",
    "    if os.path.exists('bbc_latin_america.csv'):\n",
    "        df_existing = pd.read_csv('bbc_latin_america.csv')\n",
    "        df_combined = pd.concat([df_existing, df_new])\n",
    "    else:\n",
    "        df_combined = df_new\n",
    "\n",
    "    df_combined = df_combined.drop_duplicates()\n",
    "    df_combined['main_category']='Latin America'\n",
    "    df_combined.to_csv('bbc_latin_america.csv', index=False)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63388958-d144-475c-9848-c81b47f471c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Browser opens\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#Middle east\n",
    "\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.bbc.com/news/world/middle_east'\n",
    "wait = 3\n",
    "wait2 = 5\n",
    "today = datetime.now()\n",
    "\n",
    "def convert_date(text):\n",
    "    if re.search(r\"\\b\\d+ hr\\b|\\b\\d+ hrs\\b\", text):\n",
    "        hours_ago = int(re.search(r\"(\\d+) hr?s?\", text).group(1))\n",
    "        return (today - timedelta(hours=hours_ago)).strftime('%Y-%m-%d')\n",
    "    if re.search(r\"\\b\\d+ day\\b|\\b\\d+ days\\b\", text):\n",
    "        days_ago = int(re.search(r\"(\\d+) day?s?\", text).group(1))\n",
    "        return (today - timedelta(days=days_ago)).strftime('%Y-%m-%d')\n",
    "    if re.search(r\"\\b\\d+ min\\b|\\b\\d+ mins\\b\", text):\n",
    "        minutes_ago = int(re.search(r\"(\\d+) min?s?\", text).group(1))\n",
    "        return (today - timedelta(minutes=minutes_ago)).strftime('%Y-%m-%d')\n",
    "    try:\n",
    "        return datetime.strptime(text, '%d %b %Y').strftime('%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        return text\n",
    "\n",
    "\n",
    "error = False\n",
    "chrome_check = False\n",
    "try:\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    driver.get(\"about:blank\")\n",
    "\n",
    "    driver.implicitly_wait(30)\n",
    "    chrome_check = True\n",
    "    print('Browser opens')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print('Browser does not open')\n",
    "    chrome_check = False\n",
    "\n",
    "\n",
    "news_selector = 'section > div div[data-testid=\"anchor-inner-wrapper\"]'\n",
    "title_selector = 'h2'\n",
    "subtitle_selector = 'p'\n",
    "date_selector = '[data-testid=\"card-metadata-lastupdated\"]'\n",
    "category_selector = '[data-testid=\"card-metadata-tag\"]'\n",
    "button_selector = 'div[data-testid=\"pagination\"] button'\n",
    "link_selector = 'a'\n",
    "\n",
    "\n",
    "if chrome_check:\n",
    "    driver.get(url)\n",
    "    sleep(random.uniform(wait, wait2))\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    button_tags = soup.select(button_selector)\n",
    "    loop_num = int(button_tags[-2].text)\n",
    "    d_list = []\n",
    "    for index in range(loop_num):\n",
    "        news_tags = soup.select(news_selector)\n",
    "        for i, news_tag in enumerate(news_tags):\n",
    "            if i == 2 or i == 5:\n",
    "                continue\n",
    "            title_tag = news_tag.select(title_selector)\n",
    "            subtitle_tag = news_tag.select(subtitle_selector)\n",
    "            date_tag = news_tag.select(date_selector)\n",
    "            category_tag = news_tag.select(category_selector)\n",
    "            link_tag = news_tag.select(link_selector)\n",
    "\n",
    "            title = title_tag[0].text if title_tag else ''\n",
    "            subtitle = subtitle_tag[0].text if subtitle_tag else ''\n",
    "            date = convert_date(date_tag[0].text) if date_tag else today.strftime('%Y-%m-%d')\n",
    "            category = category_tag[0].text if category_tag else ''\n",
    "            link = f\"https://www.bbc.com{link_tag[0]['href']}\" if link_tag else ''\n",
    "\n",
    "            d_list.append({\n",
    "                \"title\": title,\n",
    "                \"subtitle\": subtitle,\n",
    "                \"date\": date,\n",
    "                \"category\": category,\n",
    "                \"link\": link\n",
    "            })\n",
    "        # next_pages = driver.find_elements(By.CSS_SELECTOR,button_selector)\n",
    "        # if next_pages and index < loop_num - 1:\n",
    "        #     next_pages[-1].click()\n",
    "        #     sleep(random.uniform(wait, wait2))\n",
    "        #     soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "        # else:\n",
    "        #     break\n",
    "        try:\n",
    "            next_pages = driver.find_elements(By.CSS_SELECTOR, button_selector)\n",
    "            if next_pages and index < loop_num - 1:\n",
    "                driver.execute_script(\"arguments[0].click();\", next_pages[-1])\n",
    "                sleep(random.uniform(wait, wait2))\n",
    "                soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            else:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error clicking button with JavaScript: {e}\")\n",
    "            break\n",
    "    df_new = pd.DataFrame(d_list)\n",
    "    if os.path.exists('bbc_middle_east.csv'):\n",
    "        df_existing = pd.read_csv('bbc_middle_east.csv')\n",
    "        df_combined = pd.concat([df_existing, df_new])\n",
    "    else:\n",
    "        df_combined = df_new\n",
    "\n",
    "    df_combined = df_combined.drop_duplicates()\n",
    "    df_combined['main_category']='Middle East'\n",
    "    df_combined.to_csv('bbc_middle_east.csv', index=False)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccf3a7a9-8377-46f5-9e70-2066e728ccc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Browser opens\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#Pictures of world news\n",
    "\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.bbc.com/news/in_pictures'\n",
    "wait = 3\n",
    "wait2 = 5\n",
    "today = datetime.now()\n",
    "\n",
    "def convert_date(text):\n",
    "    if re.search(r\"\\b\\d+ hr\\b|\\b\\d+ hrs\\b\", text):\n",
    "        hours_ago = int(re.search(r\"(\\d+) hr?s?\", text).group(1))\n",
    "        return (today - timedelta(hours=hours_ago)).strftime('%Y-%m-%d')\n",
    "    if re.search(r\"\\b\\d+ day\\b|\\b\\d+ days\\b\", text):\n",
    "        days_ago = int(re.search(r\"(\\d+) day?s?\", text).group(1))\n",
    "        return (today - timedelta(days=days_ago)).strftime('%Y-%m-%d')\n",
    "    if re.search(r\"\\b\\d+ min\\b|\\b\\d+ mins\\b\", text):\n",
    "        minutes_ago = int(re.search(r\"(\\d+) min?s?\", text).group(1))\n",
    "        return (today - timedelta(minutes=minutes_ago)).strftime('%Y-%m-%d')\n",
    "    try:\n",
    "        return datetime.strptime(text, '%d %b %Y').strftime('%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        return text\n",
    "\n",
    "\n",
    "error = False\n",
    "chrome_check = False\n",
    "try:\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    driver.get(\"about:blank\")\n",
    "\n",
    "    driver.implicitly_wait(30)\n",
    "    chrome_check = True\n",
    "    print('Browser opens')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print('Browser does not open')\n",
    "    chrome_check = False\n",
    "\n",
    "\n",
    "news_selector = 'section > div div[data-testid=\"anchor-inner-wrapper\"]'\n",
    "title_selector = 'h2'\n",
    "subtitle_selector = 'p'\n",
    "date_selector = '[data-testid=\"card-metadata-lastupdated\"]'\n",
    "category_selector = '[data-testid=\"card-metadata-tag\"]'\n",
    "button_selector = 'div[data-testid=\"pagination\"] button'\n",
    "link_selector = 'a'\n",
    "\n",
    "\n",
    "if chrome_check:\n",
    "    driver.get(url)\n",
    "    sleep(random.uniform(wait, wait2))\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    button_tags = soup.select(button_selector)\n",
    "    loop_num = int(button_tags[-2].text)\n",
    "    d_list = []\n",
    "    for index in range(loop_num):\n",
    "        news_tags = soup.select(news_selector)\n",
    "        for i, news_tag in enumerate(news_tags):\n",
    "            if i == 2 or i == 5:\n",
    "                continue\n",
    "            title_tag = news_tag.select(title_selector)\n",
    "            subtitle_tag = news_tag.select(subtitle_selector)\n",
    "            date_tag = news_tag.select(date_selector)\n",
    "            category_tag = news_tag.select(category_selector)\n",
    "            link_tag = news_tag.select(link_selector)\n",
    "\n",
    "            title = title_tag[0].text if title_tag else ''\n",
    "            subtitle = subtitle_tag[0].text if subtitle_tag else ''\n",
    "            date = convert_date(date_tag[0].text) if date_tag else today.strftime('%Y-%m-%d')\n",
    "            category = category_tag[0].text if category_tag else ''\n",
    "            link = f\"https://www.bbc.com{link_tag[0]['href']}\" if link_tag else ''\n",
    "\n",
    "            d_list.append({\n",
    "                \"title\": title,\n",
    "                \"subtitle\": subtitle,\n",
    "                \"date\": date,\n",
    "                \"category\": category,\n",
    "                \"link\": link\n",
    "            })\n",
    "        # next_pages = driver.find_elements(By.CSS_SELECTOR,button_selector)\n",
    "        # if next_pages and index < loop_num - 1:\n",
    "        #     next_pages[-1].click()\n",
    "        #     sleep(random.uniform(wait, wait2))\n",
    "        #     soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "        # else:\n",
    "        #     break\n",
    "        try:\n",
    "            next_pages = driver.find_elements(By.CSS_SELECTOR, button_selector)\n",
    "            if next_pages and index < loop_num - 1:\n",
    "                driver.execute_script(\"arguments[0].click();\", next_pages[-1])\n",
    "                sleep(random.uniform(wait, wait2))\n",
    "                soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            else:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error clicking button with JavaScript: {e}\")\n",
    "            break\n",
    "    df_new = pd.DataFrame(d_list)\n",
    "    if os.path.exists('bbc_pictures.csv'):\n",
    "        df_existing = pd.read_csv('bbc_pictures.csv')\n",
    "        df_combined = pd.concat([df_existing, df_new])\n",
    "    else:\n",
    "        df_combined = df_new\n",
    "\n",
    "    df_combined = df_combined.drop_duplicates()\n",
    "    df_combined['main_category']='In Pictures'\n",
    "    df_combined.to_csv('bbc_pictures.csv', index=False)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e5497-8afa-4577-9b36-01580091a1be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdecfa04-6ca6-4c97-85fa-52a372dc0d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BBC in depth\n",
    "\n",
    "# from time import sleep\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import random\n",
    "# from datetime import datetime, timedelta\n",
    "# import re\n",
    "\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.firefox.options import Options\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# url = 'https://www.bbc.com/news/bbcindepth'\n",
    "# wait = 3\n",
    "# wait2 = 5\n",
    "# today = datetime.now()\n",
    "\n",
    "# def convert_date(text):\n",
    "#     if re.search(r\"\\b\\d+ hr\\b|\\b\\d+ hrs\\b\", text):\n",
    "#         hours_ago = int(re.search(r\"(\\d+) hr?s?\", text).group(1))\n",
    "#         return (today - timedelta(hours=hours_ago)).strftime('%Y-%m-%d')\n",
    "#     if re.search(r\"\\b\\d+ day\\b|\\b\\d+ days\\b\", text):\n",
    "#         days_ago = int(re.search(r\"(\\d+) day?s?\", text).group(1))\n",
    "#         return (today - timedelta(days=days_ago)).strftime('%Y-%m-%d')\n",
    "#     if re.search(r\"\\b\\d+ min\\b|\\b\\d+ mins\\b\", text):\n",
    "#         minutes_ago = int(re.search(r\"(\\d+) min?s?\", text).group(1))\n",
    "#         return (today - timedelta(minutes=minutes_ago)).strftime('%Y-%m-%d')\n",
    "#     try:\n",
    "#         return datetime.strptime(text, '%d %b %Y').strftime('%Y-%m-%d')\n",
    "#     except ValueError:\n",
    "#         return text\n",
    "\n",
    "\n",
    "# error = False\n",
    "# chrome_check = False\n",
    "# try:\n",
    "#     options = Options()\n",
    "#     options.add_argument('--headless')\n",
    "#     driver = webdriver.Chrome(options=options)\n",
    "\n",
    "#     driver.get(\"about:blank\")\n",
    "\n",
    "#     driver.implicitly_wait(30)\n",
    "#     chrome_check = True\n",
    "#     print('Browser opens')\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "#     print('Browser does not open')\n",
    "#     chrome_check = False\n",
    "\n",
    "\n",
    "# news_selector = 'section > div div[data-testid=\"anchor-inner-wrapper\"]'\n",
    "# title_selector = 'h2'\n",
    "# subtitle_selector = 'p'\n",
    "# date_selector = '[data-testid=\"card-metadata-lastupdated\"]'\n",
    "# category_selector = '[data-testid=\"card-metadata-tag\"]'\n",
    "# button_selector = 'div[data-testid=\"pagination\"] button'\n",
    "# link_selector = 'a'\n",
    "\n",
    "\n",
    "# if chrome_check:\n",
    "#     driver.get(url)\n",
    "#     sleep(random.uniform(wait, wait2))\n",
    "#     soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "#     button_tags = soup.select(button_selector)\n",
    "#     print(button_tags)\n",
    "#     loop_num = int(button_tags[-2].text)\n",
    "#     d_list = []\n",
    "#     for index in range(loop_num):\n",
    "#         news_tags = soup.select(news_selector)\n",
    "#         for i, news_tag in enumerate(news_tags):\n",
    "#             if i == 2 or i == 5:\n",
    "#                 continue\n",
    "#             title_tag = news_tag.select(title_selector)\n",
    "#             subtitle_tag = news_tag.select(subtitle_selector)\n",
    "#             date_tag = news_tag.select(date_selector)\n",
    "#             category_tag = news_tag.select(category_selector)\n",
    "#             link_tag = news_tag.select(link_selector)\n",
    "\n",
    "#             title = title_tag[0].text if title_tag else ''\n",
    "#             subtitle = subtitle_tag[0].text if subtitle_tag else ''\n",
    "#             date = convert_date(date_tag[0].text) if date_tag else today.strftime('%Y-%m-%d')\n",
    "#             category = category_tag[0].text if category_tag else ''\n",
    "#             link = f\"https://www.bbc.com{link_tag[0]['href']}\" if link_tag else ''\n",
    "\n",
    "#             d_list.append({\n",
    "#                 \"title\": title,\n",
    "#                 \"subtitle\": subtitle,\n",
    "#                 \"date\": date,\n",
    "#                 \"category\": category,\n",
    "#                 \"link\": link\n",
    "#             })\n",
    "#         next_pages = driver.find_elements(By.CSS_SELECTOR,button_selector)\n",
    "#         if next_pages and index < loop_num - 1:\n",
    "#             next_pages[-1].click()\n",
    "#             sleep(random.uniform(wait, wait2))\n",
    "#             soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "#         else:\n",
    "#             break\n",
    "#     df_new = pd.DataFrame(d_list)\n",
    "#     if os.path.exists('bbc_indepth.csv'):\n",
    "#         df_existing = pd.read_csv('bbc_indepth.csv')\n",
    "#         df_combined = pd.concat([df_existing, df_new])\n",
    "#     else:\n",
    "#         df_combined = df_new\n",
    "\n",
    "#     df_combined = df_combined.drop_duplicates()\n",
    "#     df_combined.to_csv('bbc_indepth.csv', index=False)\n",
    "# print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63f2fe6a-8b6c-4036-acf3-874519c7b414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Browser opens\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#BBC verify\n",
    "\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.bbc.com/news/reality_check'\n",
    "wait = 3\n",
    "wait2 = 5\n",
    "today = datetime.now()\n",
    "\n",
    "def convert_date(text):\n",
    "    if re.search(r\"\\b\\d+ hr\\b|\\b\\d+ hrs\\b\", text):\n",
    "        hours_ago = int(re.search(r\"(\\d+) hr?s?\", text).group(1))\n",
    "        return (today - timedelta(hours=hours_ago)).strftime('%Y-%m-%d')\n",
    "    if re.search(r\"\\b\\d+ day\\b|\\b\\d+ days\\b\", text):\n",
    "        days_ago = int(re.search(r\"(\\d+) day?s?\", text).group(1))\n",
    "        return (today - timedelta(days=days_ago)).strftime('%Y-%m-%d')\n",
    "    if re.search(r\"\\b\\d+ min\\b|\\b\\d+ mins\\b\", text):\n",
    "        minutes_ago = int(re.search(r\"(\\d+) min?s?\", text).group(1))\n",
    "        return (today - timedelta(minutes=minutes_ago)).strftime('%Y-%m-%d')\n",
    "    try:\n",
    "        return datetime.strptime(text, '%d %b %Y').strftime('%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        return text\n",
    "\n",
    "\n",
    "error = False\n",
    "chrome_check = False\n",
    "try:\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    driver.get(\"about:blank\")\n",
    "\n",
    "    driver.implicitly_wait(30)\n",
    "    chrome_check = True\n",
    "    print('Browser opens')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print('Browser does not open')\n",
    "    chrome_check = False\n",
    "\n",
    "\n",
    "news_selector = 'section > div div[data-testid=\"anchor-inner-wrapper\"]'\n",
    "title_selector = 'h2'\n",
    "subtitle_selector = 'p'\n",
    "date_selector = '[data-testid=\"card-metadata-lastupdated\"]'\n",
    "category_selector = '[data-testid=\"card-metadata-tag\"]'\n",
    "button_selector = 'div[data-testid=\"pagination\"] button'\n",
    "link_selector = 'a'\n",
    "\n",
    "\n",
    "if chrome_check:\n",
    "    driver.get(url)\n",
    "    sleep(random.uniform(wait, wait2))\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    button_tags = soup.select(button_selector)\n",
    "    loop_num = int(button_tags[-2].text)\n",
    "    d_list = []\n",
    "    for index in range(loop_num):\n",
    "        news_tags = soup.select(news_selector)\n",
    "        for i, news_tag in enumerate(news_tags):\n",
    "            if i == 2 or i == 5:\n",
    "                continue\n",
    "            title_tag = news_tag.select(title_selector)\n",
    "            subtitle_tag = news_tag.select(subtitle_selector)\n",
    "            date_tag = news_tag.select(date_selector)\n",
    "            category_tag = news_tag.select(category_selector)\n",
    "            link_tag = news_tag.select(link_selector)\n",
    "\n",
    "            title = title_tag[0].text if title_tag else ''\n",
    "            subtitle = subtitle_tag[0].text if subtitle_tag else ''\n",
    "            date = convert_date(date_tag[0].text) if date_tag else today.strftime('%Y-%m-%d')\n",
    "            category = category_tag[0].text if category_tag else ''\n",
    "            link = f\"https://www.bbc.com{link_tag[0]['href']}\" if link_tag else ''\n",
    "\n",
    "            d_list.append({\n",
    "                \"title\": title,\n",
    "                \"subtitle\": subtitle,\n",
    "                \"date\": date,\n",
    "                \"category\": category,\n",
    "                \"link\": link\n",
    "            })\n",
    "        # next_pages = driver.find_elements(By.CSS_SELECTOR,button_selector)\n",
    "        # if next_pages and index < loop_num - 1:\n",
    "        #     next_pages[-1].click()\n",
    "        #     sleep(random.uniform(wait, wait2))\n",
    "        #     soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "        # else:\n",
    "        #     break\n",
    "        try:\n",
    "            next_pages = driver.find_elements(By.CSS_SELECTOR, button_selector)\n",
    "            if next_pages and index < loop_num - 1:\n",
    "                driver.execute_script(\"arguments[0].click();\", next_pages[-1])\n",
    "                sleep(random.uniform(wait, wait2))\n",
    "                soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            else:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error clicking button with JavaScript: {e}\")\n",
    "            break\n",
    "    df_new = pd.DataFrame(d_list)\n",
    "    if os.path.exists('bbc_verify.csv'):\n",
    "        df_existing = pd.read_csv('bbc_verify.csv')\n",
    "        df_combined = pd.concat([df_existing, df_new])\n",
    "    else:\n",
    "        df_combined = df_new\n",
    "\n",
    "    df_combined = df_combined.drop_duplicates()\n",
    "\n",
    "    df_combined['main_category']='BBC Verify'\n",
    "\n",
    "    df_combined.to_csv('bbc_verify.csv', index=False)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "557a1eb2-1c73-4bbc-8737-37ea336db2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Browser opens\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#BBC Business Executive Lounge\n",
    "\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.bbc.com/business/executive-lounge'\n",
    "wait = 3\n",
    "wait2 = 5\n",
    "today = datetime.now()\n",
    "\n",
    "def convert_date(text):\n",
    "    if re.search(r\"\\b\\d+ hr\\b|\\b\\d+ hrs\\b\", text):\n",
    "        hours_ago = int(re.search(r\"(\\d+) hr?s?\", text).group(1))\n",
    "        return (today - timedelta(hours=hours_ago)).strftime('%Y-%m-%d')\n",
    "    if re.search(r\"\\b\\d+ day\\b|\\b\\d+ days\\b\", text):\n",
    "        days_ago = int(re.search(r\"(\\d+) day?s?\", text).group(1))\n",
    "        return (today - timedelta(days=days_ago)).strftime('%Y-%m-%d')\n",
    "    if re.search(r\"\\b\\d+ min\\b|\\b\\d+ mins\\b\", text):\n",
    "        minutes_ago = int(re.search(r\"(\\d+) min?s?\", text).group(1))\n",
    "        return (today - timedelta(minutes=minutes_ago)).strftime('%Y-%m-%d')\n",
    "    try:\n",
    "        return datetime.strptime(text, '%d %b %Y').strftime('%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        return text\n",
    "\n",
    "\n",
    "error = False\n",
    "chrome_check = False\n",
    "try:\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    driver.get(\"about:blank\")\n",
    "\n",
    "    driver.implicitly_wait(30)\n",
    "    chrome_check = True\n",
    "    print('Browser opens')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print('Browser does not open')\n",
    "    chrome_check = False\n",
    "\n",
    "\n",
    "news_selector = 'section > div div[data-testid=\"anchor-inner-wrapper\"]'\n",
    "title_selector = 'h2'\n",
    "subtitle_selector = 'p'\n",
    "date_selector = '[data-testid=\"card-metadata-lastupdated\"]'\n",
    "category_selector = '[data-testid=\"card-metadata-tag\"]'\n",
    "button_selector = 'div[data-testid=\"pagination\"] button'\n",
    "link_selector = 'a'\n",
    "\n",
    "\n",
    "if chrome_check:\n",
    "    driver.get(url)\n",
    "    sleep(random.uniform(wait, wait2))\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    button_tags = soup.select(button_selector)\n",
    "    loop_num = int(button_tags[-2].text)\n",
    "    d_list = []\n",
    "    for index in range(loop_num):\n",
    "        news_tags = soup.select(news_selector)\n",
    "        for i, news_tag in enumerate(news_tags):\n",
    "            if i == 2 or i == 5:\n",
    "                continue\n",
    "            title_tag = news_tag.select(title_selector)\n",
    "            subtitle_tag = news_tag.select(subtitle_selector)\n",
    "            date_tag = news_tag.select(date_selector)\n",
    "            category_tag = news_tag.select(category_selector)\n",
    "            link_tag = news_tag.select(link_selector)\n",
    "\n",
    "            title = title_tag[0].text if title_tag else ''\n",
    "            subtitle = subtitle_tag[0].text if subtitle_tag else ''\n",
    "            date = convert_date(date_tag[0].text) if date_tag else today.strftime('%Y-%m-%d')\n",
    "            category = category_tag[0].text if category_tag else ''\n",
    "            link = f\"https://www.bbc.com{link_tag[0]['href']}\" if link_tag else ''\n",
    "\n",
    "            d_list.append({\n",
    "                \"title\": title,\n",
    "                \"subtitle\": subtitle,\n",
    "                \"date\": date,\n",
    "                \"category\": category,\n",
    "                \"link\": link\n",
    "            })\n",
    "        # next_pages = driver.find_elements(By.CSS_SELECTOR,button_selector)\n",
    "        # if next_pages and index < loop_num - 1:\n",
    "        #     next_pages[-1].click()\n",
    "        #     sleep(random.uniform(wait, wait2))\n",
    "        #     soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "        # else:\n",
    "        #     break\n",
    "        try:\n",
    "            next_pages = driver.find_elements(By.CSS_SELECTOR, button_selector)\n",
    "            if next_pages and index < loop_num - 1:\n",
    "                driver.execute_script(\"arguments[0].click();\", next_pages[-1])\n",
    "                sleep(random.uniform(wait, wait2))\n",
    "                soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            else:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error clicking button with JavaScript: {e}\")\n",
    "            break\n",
    "    df_new = pd.DataFrame(d_list)\n",
    "    if os.path.exists('bbc_business_executivelounge.csv'):\n",
    "        df_existing = pd.read_csv('bbc_business_executivelounge.csv')\n",
    "        df_combined = pd.concat([df_existing, df_new])\n",
    "    else:\n",
    "        df_combined = df_new\n",
    "\n",
    "    df_combined = df_combined.drop_duplicates()\n",
    "    df_combined['main_category']='Business'\n",
    "\n",
    "    df_combined.to_csv('bbc_business_executivelounge.csv', index=False)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ab02d7f-8dc3-4aac-9534-34ad25d461e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Browser opens\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#BBC Business technology\n",
    "\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.bbc.com/business/technology-of-business'\n",
    "wait = 3\n",
    "wait2 = 5\n",
    "today = datetime.now()\n",
    "\n",
    "def convert_date(text):\n",
    "    if re.search(r\"\\b\\d+ hr\\b|\\b\\d+ hrs\\b\", text):\n",
    "        hours_ago = int(re.search(r\"(\\d+) hr?s?\", text).group(1))\n",
    "        return (today - timedelta(hours=hours_ago)).strftime('%Y-%m-%d')\n",
    "    if re.search(r\"\\b\\d+ day\\b|\\b\\d+ days\\b\", text):\n",
    "        days_ago = int(re.search(r\"(\\d+) day?s?\", text).group(1))\n",
    "        return (today - timedelta(days=days_ago)).strftime('%Y-%m-%d')\n",
    "    if re.search(r\"\\b\\d+ min\\b|\\b\\d+ mins\\b\", text):\n",
    "        minutes_ago = int(re.search(r\"(\\d+) min?s?\", text).group(1))\n",
    "        return (today - timedelta(minutes=minutes_ago)).strftime('%Y-%m-%d')\n",
    "    try:\n",
    "        return datetime.strptime(text, '%d %b %Y').strftime('%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        return text\n",
    "\n",
    "\n",
    "error = False\n",
    "chrome_check = False\n",
    "try:\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    driver.get(\"about:blank\")\n",
    "\n",
    "    driver.implicitly_wait(30)\n",
    "    chrome_check = True\n",
    "    print('Browser opens')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print('Browser does not open')\n",
    "    chrome_check = False\n",
    "\n",
    "\n",
    "news_selector = 'section > div div[data-testid=\"anchor-inner-wrapper\"]'\n",
    "title_selector = 'h2'\n",
    "subtitle_selector = 'p'\n",
    "date_selector = '[data-testid=\"card-metadata-lastupdated\"]'\n",
    "category_selector = '[data-testid=\"card-metadata-tag\"]'\n",
    "button_selector = 'div[data-testid=\"pagination\"] button'\n",
    "link_selector = 'a'\n",
    "\n",
    "\n",
    "if chrome_check:\n",
    "    driver.get(url)\n",
    "    sleep(random.uniform(wait, wait2))\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    button_tags = soup.select(button_selector)\n",
    "    loop_num = int(button_tags[-2].text)\n",
    "    d_list = []\n",
    "    for index in range(loop_num):\n",
    "        news_tags = soup.select(news_selector)\n",
    "        for i, news_tag in enumerate(news_tags):\n",
    "            if i == 2 or i == 5:\n",
    "                continue\n",
    "            title_tag = news_tag.select(title_selector)\n",
    "            subtitle_tag = news_tag.select(subtitle_selector)\n",
    "            date_tag = news_tag.select(date_selector)\n",
    "            category_tag = news_tag.select(category_selector)\n",
    "            link_tag = news_tag.select(link_selector)\n",
    "\n",
    "            title = title_tag[0].text if title_tag else ''\n",
    "            subtitle = subtitle_tag[0].text if subtitle_tag else ''\n",
    "            date = convert_date(date_tag[0].text) if date_tag else today.strftime('%Y-%m-%d')\n",
    "            category = category_tag[0].text if category_tag else ''\n",
    "            link = f\"https://www.bbc.com{link_tag[0]['href']}\" if link_tag else ''\n",
    "\n",
    "            d_list.append({\n",
    "                \"title\": title,\n",
    "                \"subtitle\": subtitle,\n",
    "                \"date\": date,\n",
    "                \"category\": category,\n",
    "                \"link\": link\n",
    "            })\n",
    "        # next_pages = driver.find_elements(By.CSS_SELECTOR,button_selector)\n",
    "        # if next_pages and index < loop_num - 1:\n",
    "        #     next_pages[-1].click()\n",
    "        #     sleep(random.uniform(wait, wait2))\n",
    "        #     soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "        # else:\n",
    "        #     break\n",
    "        try:\n",
    "            next_pages = driver.find_elements(By.CSS_SELECTOR, button_selector)\n",
    "            if next_pages and index < loop_num - 1:\n",
    "                driver.execute_script(\"arguments[0].click();\", next_pages[-1])\n",
    "                sleep(random.uniform(wait, wait2))\n",
    "                soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            else:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error clicking button with JavaScript: {e}\")\n",
    "            break\n",
    "    df_new = pd.DataFrame(d_list)\n",
    "    if os.path.exists('bbc_business_technology.csv'):\n",
    "        df_existing = pd.read_csv('bbc_business_technology.csv')\n",
    "        df_combined = pd.concat([df_existing, df_new])\n",
    "    else:\n",
    "        df_combined = df_new\n",
    "\n",
    "    df_combined = df_combined.drop_duplicates()\n",
    "    df_combined['main_category']='Business'\n",
    "    df_combined.to_csv('bbc_business_technology.csv', index=False)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56c14652-1aab-4ff0-8ff6-ae388a4e04bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Browser opens\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#BBC Business bbc_business_future\n",
    "\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.bbc.com/business/future-of-business'\n",
    "wait = 3\n",
    "wait2 = 5\n",
    "today = datetime.now()\n",
    "\n",
    "def convert_date(text):\n",
    "    if re.search(r\"\\b\\d+ hr\\b|\\b\\d+ hrs\\b\", text):\n",
    "        hours_ago = int(re.search(r\"(\\d+) hr?s?\", text).group(1))\n",
    "        return (today - timedelta(hours=hours_ago)).strftime('%Y-%m-%d')\n",
    "    if re.search(r\"\\b\\d+ day\\b|\\b\\d+ days\\b\", text):\n",
    "        days_ago = int(re.search(r\"(\\d+) day?s?\", text).group(1))\n",
    "        return (today - timedelta(days=days_ago)).strftime('%Y-%m-%d')\n",
    "    if re.search(r\"\\b\\d+ min\\b|\\b\\d+ mins\\b\", text):\n",
    "        minutes_ago = int(re.search(r\"(\\d+) min?s?\", text).group(1))\n",
    "        return (today - timedelta(minutes=minutes_ago)).strftime('%Y-%m-%d')\n",
    "    try:\n",
    "        return datetime.strptime(text, '%d %b %Y').strftime('%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        return text\n",
    "\n",
    "\n",
    "error = False\n",
    "chrome_check = False\n",
    "try:\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    driver.get(\"about:blank\")\n",
    "\n",
    "    driver.implicitly_wait(30)\n",
    "    chrome_check = True\n",
    "    print('Browser opens')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print('Browser does not open')\n",
    "    chrome_check = False\n",
    "\n",
    "\n",
    "news_selector = 'section > div div[data-testid=\"anchor-inner-wrapper\"]'\n",
    "title_selector = 'h2'\n",
    "subtitle_selector = 'p'\n",
    "date_selector = '[data-testid=\"card-metadata-lastupdated\"]'\n",
    "category_selector = '[data-testid=\"card-metadata-tag\"]'\n",
    "button_selector = 'div[data-testid=\"pagination\"] button'\n",
    "link_selector = 'a'\n",
    "\n",
    "\n",
    "if chrome_check:\n",
    "    driver.get(url)\n",
    "    sleep(random.uniform(wait, wait2))\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    button_tags = soup.select(button_selector)\n",
    "    loop_num = int(button_tags[-2].text)\n",
    "    d_list = []\n",
    "    for index in range(loop_num):\n",
    "        news_tags = soup.select(news_selector)\n",
    "        for i, news_tag in enumerate(news_tags):\n",
    "            if i == 2 or i == 5:\n",
    "                continue\n",
    "            title_tag = news_tag.select(title_selector)\n",
    "            subtitle_tag = news_tag.select(subtitle_selector)\n",
    "            date_tag = news_tag.select(date_selector)\n",
    "            category_tag = news_tag.select(category_selector)\n",
    "            link_tag = news_tag.select(link_selector)\n",
    "\n",
    "            title = title_tag[0].text if title_tag else ''\n",
    "            subtitle = subtitle_tag[0].text if subtitle_tag else ''\n",
    "            date = convert_date(date_tag[0].text) if date_tag else today.strftime('%Y-%m-%d')\n",
    "            category = category_tag[0].text if category_tag else ''\n",
    "            link = f\"https://www.bbc.com{link_tag[0]['href']}\" if link_tag else ''\n",
    "\n",
    "            d_list.append({\n",
    "                \"title\": title,\n",
    "                \"subtitle\": subtitle,\n",
    "                \"date\": date,\n",
    "                \"category\": category,\n",
    "                \"link\": link\n",
    "            })\n",
    "        # next_pages = driver.find_elements(By.CSS_SELECTOR,button_selector)\n",
    "        # if next_pages and index < loop_num - 1:\n",
    "        #     next_pages[-1].click()\n",
    "        #     sleep(random.uniform(wait, wait2))\n",
    "        #     soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "        # else:\n",
    "        #     break\n",
    "        try:\n",
    "            next_pages = driver.find_elements(By.CSS_SELECTOR, button_selector)\n",
    "            if next_pages and index < loop_num - 1:\n",
    "                driver.execute_script(\"arguments[0].click();\", next_pages[-1])\n",
    "                sleep(random.uniform(wait, wait2))\n",
    "                soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            else:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error clicking button with JavaScript: {e}\")\n",
    "            break\n",
    "    df_new = pd.DataFrame(d_list)\n",
    "    if os.path.exists('bbc_business_future.csv'):\n",
    "        df_existing = pd.read_csv('bbc_business_future.csv')\n",
    "        df_combined = pd.concat([df_existing, df_new])\n",
    "    else:\n",
    "        df_combined = df_new\n",
    "\n",
    "    df_combined = df_combined.drop_duplicates()\n",
    "    df_combined['main_category']='Business'\n",
    "    df_combined.to_csv('bbc_business_future.csv', index=False)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdfdb42-676a-40ab-9b78-8e6ae10f6289",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
